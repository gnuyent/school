{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning \n",
    "# Assignment 2: Linear Regression Model\n",
    "\n",
    "**Author:** Yang Xu, Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Semester:** Spring 2022\n",
    "\n",
    "**Total: 10 points**\n",
    "\n",
    "The goal of this homework assignment is to practice the Python implementation of two methods for training Linear Regression models: **normal equations** and **gradient descent**. See detailed instructions below. \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "The task is to build a linear regression model that predicts the GPAs of university students from two features, Math SAT and Verb SAT. \n",
    "- Task 1) Train the model using Normal Equation method.\n",
    "- Task 2) Train the model using Gradient Descent method.\n",
    "- Task 3) Play around with different learning rate $\\alpha$s. \n",
    "\n",
    "## Datasets\n",
    "The file *sat_gpa.csv* contains all training and testing data. It has 105 rows and 3 columns. Each row is the record of a student. The three columns are <u>Math SAT score</u>, <u>Verb SAT score</u>, and <u>University GPA</u>. The first two columns are the features, and the third is the output. All data points are used as the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Use `pip install matplotlib` in command line if matplotlib is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original data: (105, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1\n",
    "**3 points**\n",
    "\n",
    "Implement the Normal Equation method for linear regression: $\\theta = (X^T X)^{-1}X^T y$\n",
    "\n",
    "Use the learned $\\theta$ to make predictions: $\\hat{y} = X\\theta$\n",
    "\n",
    "Compute the residual sum of squares of the model: $RSS = \\sum_i (\\hat{y}^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Last 2 dimensions of the array must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m data_norm[:, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#### END YOUR CODE ####\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#### START YOUR CODE ####\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# theta_method1 = (1/(inv(X) * X)) * inv(X) * y\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m theta_method1 \u001b[38;5;241m=\u001b[39m \u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#### END YOUR CODE ####\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use the theta obtained to make predictions and compute the residuals\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#### START YOUR CODE ####\u001b[39;00m\n\u001b[1;32m     25\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Projects/school/cs549/.venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:540\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    538\u001b[0m a, wrap \u001b[38;5;241m=\u001b[39m _makearray(a)\n\u001b[1;32m    539\u001b[0m _assert_stacked_2d(a)\n\u001b[0;32m--> 540\u001b[0m \u001b[43m_assert_stacked_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n\u001b[1;32m    543\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Projects/school/cs549/.venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:203\u001b[0m, in \u001b[0;36m_assert_stacked_square\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    201\u001b[0m m, n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m n:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast 2 dimensions of the array must be square\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Last 2 dimensions of the array must be square"
     ]
    }
   ],
   "source": [
    "# Create matrix X and y\n",
    "# X has three columns: \n",
    "#   - The first column contain all 1s, which is for the intercept\n",
    "#   - The second and third columns contain features, i.e., the 1st and 2nd columns of data_norm\n",
    "# y has one column, i.e., the 3rd column of data_norm\n",
    "\n",
    "X = np.ones_like(data_norm)\n",
    "#### START YOUR CODE ####\n",
    "X[:, 1:3] = data_norm[:, 1:3]\n",
    "y = data_norm[:, 2]\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Compute theta using normal equation method\n",
    "# Hint: use the inv() function imported from numpy.linalg\n",
    "#### START YOUR CODE ####\n",
    "theta_method1 = \n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Use the theta obtained to make predictions and compute the residuals\n",
    "# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\n",
    "#### START YOUR CODE ####\n",
    "y_hat = 1\n",
    "RSS1 = 1\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "# Compute residuals\n",
    "\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "print('Theta obtained from normal equation:', theta_method1)\n",
    "print('Residual sum of squares (RSS): ', RSS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected ouput\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Theta obtained from normal equation: | [-0.06234478  0.62017319  0.43647674]\n",
    "Residual sum of squares (RSS): | 0.7590471383029533\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2\n",
    "**6 points**\n",
    "\n",
    "Implement the Gradient Descent method for linear regression.\n",
    "\n",
    "The cost function: $J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{2m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_i (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Gradients w.r.t. parameters: $\\frac{\\partial J}{\\partial \\theta} = \\begin{cases}\\frac{\\partial J}{\\partial \\theta_0}\\\\ \\frac{\\partial J}{\\partial \\theta_1}\\\\ \\frac{\\partial J}{\\partial \\theta_2}\\\\ \\end{cases} = \\begin{cases}\\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_1^{(i)}\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_2^{(i)}\\\\\\end{cases}$\n",
    "\n",
    "The formula to update parameters at each iteration: $\\theta := \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Note that $X$, $y$, and $\\theta$ are all vectors (numpy arrays), and thus the operations above should be implemented in a vectorized fashion. Use `numpy.sum()`, `numpy.dot()` and other vectorized functions, and avoid writing `for` loops in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,3); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (3,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, RSS, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        RSS - residual sum of squares\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions\n",
    "        # Shape of y_hat: m by 1\n",
    "        y_hat = None\n",
    "        \n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y)\n",
    "        diff = None\n",
    "\n",
    "        # Compute the cost\n",
    "        # Hint: Use the diff computed above\n",
    "        cost = None\n",
    "        cost_array.append(cost)\n",
    "\n",
    "        # Compute gradients\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "        gradients = None\n",
    "\n",
    "        # Update theta\n",
    "        theta = None\n",
    "        #### END YOUR CODE ####\n",
    "    \n",
    "    # Compute residuals\n",
    "    # Hint: Should use the same code as Task 1\n",
    "    #### START YOUR CODE ####\n",
    "    y_hat = None\n",
    "    RSS = None\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, RSS, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.05\n",
    "MAX_ITER = 500\n",
    "\n",
    "# Initialize theta to [0,0,0]\n",
    "theta = np.zeros(3)\n",
    "theta_method2, RSS2, cost_array = gradientDescent(X, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_method2)\n",
    "print('Residual sum of squares (RSS): ', RSS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Theta obtained from gradient descent: | [0.29911574 0.32224209 0.31267172]\n",
    "Residual sum of squares (RSS): | 0.8641600584370602\n",
    "\n",
    "\n",
    "**NOTE**: It seems that the model trained with gradient descent has larger RSS than the one obtained with normal equation method.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3\n",
    "**1 point**\n",
    "\n",
    "Plot the cost against iteration number. This is a common method of examining the performance of gradient descent.\n",
    "\n",
    "Try different values of learning rate, for example, $\\alpha=\\{0.01, 0.005, 0.001\\}$, and see how the cost curves change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
