{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 8: Convolutional Neural Network (Part 3) -- Use pre-trained ResNet model\n",
    "\n",
    "**Author:** Yang Xu, Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will implement a more powerful classification model by building on top of a pre-trained ResNet model, and test its performance on the same FashionMNST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load data\n",
    "\n",
    "Load the FashionMNIST dataset provided by PyTorch. You can also change the `download` param to `False`, and copy the \"data\" folder used in the previous assignment to the current folder. See <https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader> for more information.\n",
    "\n",
    "We also need to preprocess the dataset using a customized transform function, because the ResNet implemented in torchvision take colored image as input, which has 3 channels (RGB). Thus, we repeat the single-channel grey scale  image 3 times to fit the torchvision model, using a `lambda` expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            ])\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, # True\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # False\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examine data size\n",
    "\n",
    "Now, you can examine the size of the training/test data, which is important for determining some of the parameters of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  torch.Size([64, 3, 28, 28])\n",
      "Y.shape:  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(train_loader):\n",
    "    if i > 0:\n",
    "        break\n",
    "\n",
    "print('X.shape: ', X.shape)\n",
    "print('Y.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "X.shape:  torch.Size([64, 3, 28, 28])\n",
    "y.shape:  torch.Size([64])\n",
    "\n",
    "**Note** that the number of channels becomes 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1. Build the extractor model using pre-trained ResNet model\n",
    "\n",
    "We can directly download and use the pretrained ResNet model by calling the constructor `torchvision.models.resnet18()` (or other versions of ResNet), with argument `pretrained=True`.\n",
    "\n",
    "Then we will use the components of the pretrained model as a **feature extractor**. The feature extractor is a separate class from the final classification model, because the best learning rates (and other hyper-parameters) for training the two models can be different.\n",
    "\n",
    "First, let's implement the feature extractor.\n",
    "\n",
    "**Points: 5**\n",
    "\n",
    "*Hint*: Call `torchvision.models.resnet18()` in the `__init__()` function, and use copy the components in the pretrained model into the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetFeatrueExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetFeatrueExtractor, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        resnet18_model = torchvision.models.resnet18(pretrained=True) # Call torchvision.models.resnet18()\n",
    "\n",
    "        self.conv1 = resnet18_model.conv1 # Complete all the lines below following the same naming pattern: self.NAME = resnet18_model.NAME\n",
    "        self.bn1 = resnet18_model.bn1\n",
    "        self.relu = resnet18_model.relu\n",
    "        self.maxpool = resnet18_model.maxpool\n",
    "        self.layer1 = resnet18_model.layer1\n",
    "        self.layer2 = resnet18_model.layer2\n",
    "        self.layer3 = resnet18_model.layer3\n",
    "        self.layer4 = resnet18_model.layer4\n",
    "        self.avgpool = resnet18_model.avgpool\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        # Call all the components consecutively, i.e., conv1 -> bn1 -> relu -> maxpool -> ...\n",
    "        x = self.avgpool(\n",
    "            self.layer4(\n",
    "                self.layer3(\n",
    "                    self.layer2(\n",
    "                        self.layer1(\n",
    "                            self.maxpool(\n",
    "                                self.relu(\n",
    "                                    self.bn1(\n",
    "                                        self.conv1(x)\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ) # There can be multiple lines\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size(): torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "torch.manual_seed(0)\n",
    "extractor = ResNetFeatrueExtractor()\n",
    "\n",
    "input_data = torch.randn(64, 3, 28, 28)\n",
    "output = extractor(input_data)\n",
    "\n",
    "print('output.size():', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "output.size(): torch.Size([64, 512])\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2. Build the classifier model\n",
    "\n",
    "**Points: 2**\n",
    "\n",
    "The classifier model takes the output from the extractor, and feed it to a fully connected layer and then a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.fc = nn.Linear(in_features=512, out_features=10) # Specify the in_features and out_features correctly\n",
    "        self.output = nn.LogSoftmax(dim=1) # Call nn.LogSoftmax()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        out = self.output(self.fc(x))\n",
    "        ### END YOUR CODE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size(): torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "torch.manual_seed(0)\n",
    "clasifier = ResNetClassifier()\n",
    "\n",
    "input_data = torch.randn(64, 512)\n",
    "output = clasifier(input_data)\n",
    "\n",
    "print('output.size():', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "output.size(): torch.Size([64, 10])\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3. Implement the training and test loops\n",
    "\n",
    "**Points: 3**\n",
    "\n",
    "Implement the training and test loop functions. Note that\n",
    "- Image data are input for `extractor`, whose output is the input for `classifier`.\n",
    "- Two optimizers need to be used, one for `extractor`, and the other for `classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, extractor, classifier, loss_fn, optimizer_ext, optimizer_cls, verbose=True):\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        ### START YOUR CODE ###\n",
    "        # Extract features using extractor\n",
    "        features = extractor(X)\n",
    "\n",
    "        # Feed the features to classifier\n",
    "        pred = classifier(features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, y)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Backpropagation\n",
    "        ### START YOUR CODE ###\n",
    "        # Clear the gradients for the TWO optimizers!\n",
    "        optimizer_ext.zero_grad()\n",
    "        optimizer_cls.zero_grad()\n",
    "\n",
    "        # Call backward()\n",
    "        loss.backward() # backward()\n",
    "\n",
    "        # Call step() for the TWO optimizers!\n",
    "        optimizer_ext.step()\n",
    "        optimizer_cls.step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        if verbose and i % 10 == 0:\n",
    "            loss = loss.item()\n",
    "            current_step = i * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current_step:>5d}/{len(dataloader.dataset):>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(dataloader, extractor, classifier, loss_fn):\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        ### START YOUR CODE ###\n",
    "        # The code for computing loss is similar to train_loop()\n",
    "        features = extractor(X)\n",
    "        pred = classifier(features)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        correct += torch.mean((torch.argmax(pred, dim=1) == y).float()) # Add the number of correct prediction in the current batch to `correct`\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc = correct / len(dataloader.dataset)\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, run the training and test loop functions. Training a ResNet model is very slow on CPU, so we just try 1 epoch here, and be patient while you are waiting for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.405839  [    0/60000]\n",
      "loss: 0.730246  [  640/60000]\n",
      "loss: 0.860524  [ 1280/60000]\n",
      "loss: 0.670298  [ 1920/60000]\n",
      "loss: 0.623385  [ 2560/60000]\n",
      "loss: 0.654261  [ 3200/60000]\n",
      "loss: 0.539835  [ 3840/60000]\n",
      "loss: 0.577238  [ 4480/60000]\n",
      "loss: 0.524115  [ 5120/60000]\n",
      "loss: 0.397807  [ 5760/60000]\n",
      "loss: 0.490525  [ 6400/60000]\n",
      "loss: 0.423573  [ 7040/60000]\n",
      "loss: 0.503942  [ 7680/60000]\n",
      "loss: 0.500117  [ 8320/60000]\n",
      "loss: 0.559954  [ 8960/60000]\n",
      "loss: 0.498742  [ 9600/60000]\n",
      "loss: 0.391623  [10240/60000]\n",
      "loss: 0.372260  [10880/60000]\n",
      "loss: 0.581777  [11520/60000]\n",
      "loss: 0.440633  [12160/60000]\n",
      "loss: 0.303565  [12800/60000]\n",
      "loss: 0.359001  [13440/60000]\n",
      "loss: 0.474043  [14080/60000]\n",
      "loss: 0.452061  [14720/60000]\n",
      "loss: 0.461027  [15360/60000]\n",
      "loss: 0.570573  [16000/60000]\n",
      "loss: 0.486111  [16640/60000]\n",
      "loss: 0.355791  [17280/60000]\n",
      "loss: 0.560113  [17920/60000]\n",
      "loss: 0.326118  [18560/60000]\n",
      "loss: 0.387523  [19200/60000]\n",
      "loss: 0.541789  [19840/60000]\n",
      "loss: 0.394374  [20480/60000]\n",
      "loss: 0.237777  [21120/60000]\n",
      "loss: 0.357135  [21760/60000]\n",
      "loss: 0.241448  [22400/60000]\n",
      "loss: 0.548407  [23040/60000]\n",
      "loss: 0.232309  [23680/60000]\n",
      "loss: 0.384763  [24320/60000]\n",
      "loss: 0.483747  [24960/60000]\n",
      "loss: 0.505975  [25600/60000]\n",
      "loss: 0.393593  [26240/60000]\n",
      "loss: 0.347683  [26880/60000]\n",
      "loss: 0.328927  [27520/60000]\n",
      "loss: 0.333273  [28160/60000]\n",
      "loss: 0.385503  [28800/60000]\n",
      "loss: 0.625061  [29440/60000]\n",
      "loss: 0.521628  [30080/60000]\n",
      "loss: 0.386237  [30720/60000]\n",
      "loss: 0.183004  [31360/60000]\n",
      "loss: 0.419698  [32000/60000]\n",
      "loss: 0.279642  [32640/60000]\n",
      "loss: 0.203889  [33280/60000]\n",
      "loss: 0.422873  [33920/60000]\n",
      "loss: 0.420345  [34560/60000]\n",
      "loss: 0.592962  [35200/60000]\n",
      "loss: 0.404131  [35840/60000]\n",
      "loss: 0.472470  [36480/60000]\n",
      "loss: 0.310937  [37120/60000]\n",
      "loss: 0.248253  [37760/60000]\n",
      "loss: 0.314752  [38400/60000]\n",
      "loss: 0.280461  [39040/60000]\n",
      "loss: 0.296701  [39680/60000]\n",
      "loss: 0.330726  [40320/60000]\n",
      "loss: 0.308028  [40960/60000]\n",
      "loss: 0.322709  [41600/60000]\n",
      "loss: 0.352448  [42240/60000]\n",
      "loss: 0.338844  [42880/60000]\n",
      "loss: 0.214658  [43520/60000]\n",
      "loss: 0.348849  [44160/60000]\n",
      "loss: 0.411885  [44800/60000]\n",
      "loss: 0.218842  [45440/60000]\n",
      "loss: 0.630027  [46080/60000]\n",
      "loss: 0.474644  [46720/60000]\n",
      "loss: 0.289871  [47360/60000]\n",
      "loss: 0.359024  [48000/60000]\n",
      "loss: 0.422006  [48640/60000]\n",
      "loss: 0.328406  [49280/60000]\n",
      "loss: 0.408960  [49920/60000]\n",
      "loss: 0.219627  [50560/60000]\n",
      "loss: 0.294201  [51200/60000]\n",
      "loss: 0.278264  [51840/60000]\n",
      "loss: 0.337154  [52480/60000]\n",
      "loss: 0.402841  [53120/60000]\n",
      "loss: 0.207138  [53760/60000]\n",
      "loss: 0.277330  [54400/60000]\n",
      "loss: 0.175670  [55040/60000]\n",
      "loss: 0.362016  [55680/60000]\n",
      "loss: 0.312133  [56320/60000]\n",
      "loss: 0.268353  [56960/60000]\n",
      "loss: 0.271295  [57600/60000]\n",
      "loss: 0.148000  [58240/60000]\n",
      "loss: 0.331761  [58880/60000]\n",
      "loss: 0.488819  [59520/60000]\n",
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 0.347196 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "extractor = ResNetFeatrueExtractor()\n",
    "classifier = ResNetClassifier()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "loss_fn = nn.NLLLoss() # Specify the loss function correctly\n",
    "optimizer_ext = torch.optim.Adam(params=extractor.parameters(), lr=learning_rate) # Use Adam optimizer on extractor\n",
    "optimizer_cls = torch.optim.Adam(params=classifier.parameters(), lr=learning_rate) # Use Adam optimizer on classifier\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, extractor, classifier, loss_fn, optimizer_ext, optimizer_cls, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, extractor, classifier, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Although it is just 1 epoch, you can find that the performance is pretty nice. It shows that using a pretrained model is a good strategy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
