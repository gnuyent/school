{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 11: Transformer and Transformer-based Models\n",
    "\n",
    "**Author:** Yang Xu, Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder.\n",
    "\n",
    "2) Play with the transformer-based models provided in **transformers** for multiple natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Play with transformer-based models\n",
    "**Points: 5**\n",
    "\n",
    "### 2.1 Installation\n",
    "Install the *transformers* package with the following command:\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "After it is done, you can load some pretrained BERT models and tokenizers like this (you can ignore the warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing inputs\n",
    "\n",
    "Run the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "torch.Size([1, 275])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic\n",
    "resources for human beings. From a thermodynamic point of view, any energy conversion systems\n",
    "that receive energy from the sun and/or dissipate energy to the universe are heat engines with\n",
    "photons as the \"working fluid\" and can be analyzed using the concept of entropy. While entropy\n",
    "analysis provides a particularly convenient way to understand the efficiency limits, it is typically\n",
    "taught in the context of thermodynamic cycles among quasi-equilibrium states and its\n",
    "generalization to solar energy conversion systems running in a continuous and non-equilibrium\n",
    "fashion is not straightforward. In this educational article, we present a few examples to illustrate\n",
    "how the concept of photon entropy, combined with the radiative transfer equation, can be used to\n",
    "analyze the local entropy generation processes and the efficiency limits of different solar energy\n",
    "conversion systems. We provide explicit calculations for the local and total entropy generation\n",
    "rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily\n",
    "reproduced by students. We further discuss the connection between the entropy generation and the\n",
    "device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical\n",
    "efficiency limit.\"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(len(text.split()))\n",
    "print(encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain why the `encoded_input` has more elements than the actual number of words in `text`?\\\n",
    "(**Points: 1**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your answer within the quotes ###\n",
    "answer = \"\"\"\n",
    "encoded_input has more elements that the actual number of words in text because\n",
    "there are special tokens added according to the model (such as [CLS]). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*: there is no expected output for this question.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Output word vectors from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 768])\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following code, you can find the corresponding token of each integer id in `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 2980, 2791, 1997, 1996, 3103, 1998, 1996, 3147]\n",
      "['[CLS]', 'the', 'hot', '##ness', 'of', 'the', 'sun', 'and', 'the', 'cold']\n"
     ]
    }
   ],
   "source": [
    "input_ids_pt = encoded_input['input_ids']\n",
    "input_ids_list = input_ids_pt.tolist()[0]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "\n",
    "print(input_ids_list[:10])\n",
    "print(input_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the output vector**s** among `last_hidden_state` that correpond to the input word \"entropy\"?\\\n",
    "Do they have the same values?\\\n",
    "**(Points: 1)**\n",
    "\n",
    "*Hint*: You can use a `if` statement to check if the current token is the word \"entropy\", and if so, you can append it to `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"entropy\": 6\n",
      "Do they have the same value? [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i, token in enumerate(input_tokens):\n",
    "    ### START YOUR CODE ###\n",
    "    if token == \"entropy\":\n",
    "        vectors.append(last_hidden_state[0][i]) # Replace this line with your code\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Do not change the code below\n",
    "print('Number of \"entropy\":', len(vectors))\n",
    "\n",
    "matches = [torch.allclose(vectors[i], vectors[i+1]) for i in range(len(vectors)-1)]\n",
    "print(f'Do they have the same value? {matches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** \\\n",
    "Number of \"entropy\": 6\\\n",
    "Do they have the same value? [False, False, False, False, False]\n",
    "\n",
    "---\n",
    "### 2.4 Sentence vectors from BERT\n",
    "\n",
    "We can obtain the output vectors for a batch of sentences.\n",
    "\n",
    "First, we need to break the text into a list of sentences, using a simple end-of-sentence str '.' as a separater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting in 6 sentences:\n",
      "['The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic resources for human beings.', 'From a thermodynamic point of view, any energy conversion systems that receive energy from the sun and/or dissipate energy to the universe are heat engines with photons as the \"working fluid\" and can be analyzed using the concept of entropy.', 'While entropy analysis provides a particularly convenient way to understand the efficiency limits, it is typically taught in the context of thermodynamic cycles among quasi-equilibrium states and its generalization to solar energy conversion systems running in a continuous and non-equilibrium fashion is not straightforward.', 'In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.', 'We provide explicit calculations for the local and total entropy generation rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily reproduced by students.', 'We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.replace('\\n', ' ').split('.')\n",
    "sentences = [s.strip() + '.' for s in sentences if len(s.strip())>0] # Some cleaning work\n",
    "\n",
    "print(f'Resulting in {len(sentences)} sentences:')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use tokenizer on this batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57])\n",
      "tensor([  101,  1996,  2980,  2791,  1997,  1996,  3103,  1998,  1996,  3147,\n",
      "         2791,  1997,  1996,  6058,  2686,  2024,  1999, 10288, 13821,  3775,\n",
      "         3468,  1996, 10867,  7716, 18279,  7712,  4219,  2005,  2529,  9552,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(encoded_sentences['input_ids'].shape)\n",
    "print(encoded_sentences['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find that shorter sentences are padded with a special id `0`.\n",
    "\n",
    "Next, we can obtain the output tensors for all input sentences, also in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoded_sentences)\n",
    "\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first dimension of `outputs['last_hidden_state']` is batch size. So the output tensor for the 1st sentence is `outputs['last_hidden_state'][0]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs['last_hidden_state'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each output tensor, the first 768-dim vector (at position 0) always corresponds to the special input token `[CLS]`. We can use this vector to represent the meaning of the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "CLS_vec = outputs['last_hidden_state'][0][0]\n",
    "print(CLS_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your task to compute the cosine similarities between each pair of the 6 sentences, and find the pair that has the closest meanings.\\\n",
    "**(Points: 3)**\n",
    "\n",
    "*Hint*: You can use the `cosine_similarity()` function imported at the beginning, which takes input two tensors and returns the similarity score in a tensor. So you will need to append a `.item()` to retrieve the numeric value from the returned tensor. You also need to specify the argument `dim=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <-> 1: 0.8591639995574951\n",
      "0 <-> 2: 0.7771983742713928\n",
      "0 <-> 3: 0.7985226511955261\n",
      "0 <-> 4: 0.7754687070846558\n",
      "0 <-> 5: 0.8052165508270264\n",
      "1 <-> 2: 0.8763416409492493\n",
      "1 <-> 3: 0.8321620225906372\n",
      "1 <-> 4: 0.8238450288772583\n",
      "1 <-> 5: 0.8492752909660339\n",
      "2 <-> 3: 0.8241375684738159\n",
      "2 <-> 4: 0.8598628044128418\n",
      "2 <-> 5: 0.8579832911491394\n",
      "3 <-> 4: 0.9018083810806274\n",
      "3 <-> 5: 0.9291438460350037\n",
      "4 <-> 5: 0.9185265898704529\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(i+1, 6):\n",
    "        ### START YOUR CODE ###\n",
    "        vec_i = outputs['last_hidden_state'][i][0]\n",
    "        vec_j = outputs['last_hidden_state'][j][0]\n",
    "        sim = cosine_similarity(vec_i, vec_j, dim=0).item() # Hint: when you call cosine_similarity() function, remember to specify dim=0. Also, you need append .item() at the end to obtain a number instead of a tensor.\n",
    "        ### END YOUR CODE ###\n",
    "        print(f'{i} <-> {j}: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\\\n",
    "0 <-> 1: 0.8591639399528503\\\n",
    "0 <-> 2: 0.777198314666748\\\n",
    "0 <-> 3: 0.7985224723815918\\\n",
    "0 <-> 4: 0.7754684090614319\\\n",
    "0 <-> 5: 0.8052163124084473\\\n",
    "1 <-> 2: 0.876341700553894\\\n",
    "1 <-> 3: 0.8321619629859924\\\n",
    "1 <-> 4: 0.823844850063324\\\n",
    "1 <-> 5: 0.8492751717567444\\\n",
    "2 <-> 3: 0.8241377472877502\\\n",
    "2 <-> 4: 0.8598626852035522\\\n",
    "2 <-> 5: 0.8579834699630737\\\n",
    "3 <-> 4: 0.9018082618713379\\\n",
    "3 <-> 5: 0.929144024848938\\\n",
    "4 <-> 5: 0.9185266494750977\n",
    "\n",
    "---\n",
    "You can print out the two sentences to see if the similarity score makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.\n",
      "We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.5 Play with summarization\n",
    "\n",
    "Lastly, let's play with the summarization pipelien provided by transformers. Be patient when the model is downloading. \n",
    "\n",
    "You can try the following code with different input text or arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The hotness of the sun and the coldness of outer space are inexhaustible thermodynamic resources for human beings . From a thermodynamic point of view, any energy conversion systems that receive energy from the sun or dissipate energy to the universe are heat engines with photons as the \"working fluid\"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "print(summarizer(text, max_length=150, min_length=30))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
