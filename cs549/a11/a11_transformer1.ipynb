{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 11: Transformer and Transformer-based Models\n",
    "\n",
    "**Author:** Yang Xu, Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Implement the multiple head attention sub layer\n",
    "**Points: 5**\n",
    "\n",
    "### 1.1 Initialize input data\n",
    "Step 1, generate some random input data in the shape of $\\text{n\\_inputs}\\times \\text{d\\_model}$. *Hint*: Use `np.random.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Do not remove this line\n",
    "\n",
    "d_model = 512\n",
    "n_inputs = 3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "x = np.random.rand(n_inputs, d_model)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\n",
      " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\n",
      " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\n",
      "x.shape: (3, 512)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('x:', x)\n",
    "print('x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\\\n",
    "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\\\n",
    " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\\\n",
    " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\\\n",
    "x.shape: (3, 512)\n",
    "\n",
    "---\n",
    "### 1.2 Create weight matrices for *query*, *key*, and *value*\n",
    "\n",
    "Step 2, create the weight matrices into the correct dimensions. \n",
    "\n",
    "Let's start with `W_query` and `Q`. *Hint*: We first initialize an empty tensor `W` in the dimension of `(d_model, d_k)`, using the `torch.empty()` function. Then we initialize it with `nn.init.xavier_uniform_()`.\n",
    "\n",
    "After `W_query` is initialized, we can get the query matrix `Q` with a multiplication between `x` and `W_query`. *Hint*: Use `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k)) # Create an empty tensor W with the correct dimension.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W) # Randomly initialize the values in the tensor.\n",
    "W_query = W.data.numpy() # Copy out the numpy array\n",
    "\n",
    "### START YOUR CODE ###\n",
    "Q = np.matmul(x, W_query)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\n",
      "W_query.shape: (512, 64)\n",
      "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\n",
      "Q.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('W_query[0,:5]:', W_query[0,:5])\n",
    "print('W_query.shape:', W_query.shape)\n",
    "print('Q[0, :5]:', Q[0,:5])\n",
    "print('Q.shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\\\n",
    "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\\\n",
    "W_query.shape: (512, 64)\\\n",
    "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\\\n",
    "Q.shape: (3, 64)\n",
    "\n",
    "---\n",
    "Next, repeat for `W_key` & `K`, and `W_value` & `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k)) # Create an empty tensor W with the correct dimension.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_key = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "K = np.matmul(x, W_key)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k)) # Create an empty tensor W with the correct dimension.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_value = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "V = np.matmul(x, W_value)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\n",
      "K.shape (3, 64)\n",
      "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\n",
      "V.shape (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('K[0,:5]', K[0,:5])\n",
    "print('K.shape', K.shape)\n",
    "print('V[0,:5]', V[0,:5])\n",
    "print('V.shape', V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\\\n",
    "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\\\n",
    "K.shape (3, 64)\\\n",
    "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\\\n",
    "V.shape (3, 64)\n",
    "\n",
    "---\n",
    "### 1.3 Compute attention scores and weighted output\n",
    "\n",
    "Step 3, compute the attension scores using the matrices `Q` and `K`, following the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "\n",
    "in which $\\sqrt{d_k}$ is for normalization purpose.\n",
    "\n",
    "*Hint*: You should first compute `attn_scores`, which is the unnormalized score. Then you can apply the `softmax()` function imported from `scipy` to calculate the normalized scores. Note that you need to specify the `axis` argument correctly when you call `softmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "attn_scores = (Q.dot(K.transpose()))/(np.sqrt(d_k))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "### START YOUR CODE ###\n",
    "attn_scores_norm = softmax(attn_scores, axis=1)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores.shape: (3, 3)\n",
      "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\n",
      " [ 0.23777018 -0.70730381 -0.37639239]\n",
      " [ 0.21608578 -0.73905372 -0.89881112]]\n",
      "Normalized atten_scores: [[0.36838498 0.29700212 0.33461289]\n",
      " [0.51820328 0.20140013 0.2803966 ]\n",
      " [0.58387084 0.22464925 0.19147991]]\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('attn_scores.shape:', attn_scores.shape)\n",
    "print('Unnormalized attn_scores:', attn_scores)\n",
    "print('Normalized atten_scores:', attn_scores_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\\\n",
    "attn_scores.shape: (3, 3)\\\n",
    "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\\\n",
    " [ 0.23777018 -0.70730381 -0.37639239]\\\n",
    " [ 0.21608578 -0.73905372 -0.89881112]]\\\n",
    "Normalized atten_scores: [[0.36838498 0.29700212 0.33461289]\\\n",
    " [0.51820328 0.20140013 0.2803966 ]\\\n",
    " [0.58387084 0.22464925 0.19147991]]\\\n",
    "\n",
    "---\n",
    "\n",
    "Step 4, finally, compute the output as the weighted sum of value (`V`), using the above computed `attn_scores_norm` as the weight.\n",
    "\n",
    "*Hint*: `attn_scores_norm[0,:]` is the weight for the first output `weighted_output[0,:]`, \\\n",
    "so the computation is:\\\n",
    "`weighted_output[0,:] = attn_scores_norm[0,0] * V[0,:] + attn_scores_norm[0,1] * V[1,:] + attn_scores_norm[0,2] * V[2,:]`. \\\n",
    "But you can achieve this with one line code using `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\n",
      "weighted_output.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "weighted_output = attn_scores_norm @ V\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('weighted_output[0,:5]:', weighted_output[0,:5])\n",
    "print('weighted_output.shape:', weighted_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\\\n",
    "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\\\n",
    "weighted_output.shape: (3, 64)\n",
    "\n",
    "---\n",
    "**Congratulation!** You have finished Task 1, and now you know how to implement the self-attention module, which is the core technique of Transformer."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
